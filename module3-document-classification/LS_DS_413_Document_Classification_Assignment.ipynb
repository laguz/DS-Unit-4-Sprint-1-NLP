{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S1-NLP-DS11",
      "language": "python",
      "name": "u4-s1-nlp-ds11"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "LS_DS_413_Document_Classification_Assignment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_AZPTJqmwjf"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 1, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVt44Vgpmwjh"
      },
      "source": [
        "# Document Classification (Assignment)\n",
        "\n",
        "This notebook is for you to practice skills during lecture.\n",
        "\n",
        "Today's guided module project and assignment will be different. You already know how to do classification. You ready know how to extract features from documents. So? That means you're ready to combine and practice those skills in a kaggle competition. We we will open with a five minute sprint explaining the competition, and then give you 25 minutes to work. After those twenty five minutes are up, I will give a 5-minute demo an NLP technique that will help you with document classification (*and **maybe** the competition*).\n",
        "\n",
        "Today's all about having fun and practicing your skills.\n",
        "\n",
        "## Sections\n",
        "* <a href=\"#p1\">Part 1</a>: Text Feature Extraction & Classification Pipelines\n",
        "* <a href=\"#p2\">Part 2</a>: Latent Semantic Indexing\n",
        "* <a href=\"#p3\">Part 3</a>: Word Embeddings with Spacy\n",
        "* <a href=\"#p4\">Part 4</a>: Post Lecture Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apAzD2N0mwjj"
      },
      "source": [
        "# Text Feature Extraction & Classification Pipelines (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "bOKey2J5mwjj"
      },
      "source": [
        "## Follow Along \n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model (try using the pipe method I just demoed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK2Cp2RNmwjk"
      },
      "source": [
        "### Load Competition Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjEYHH7hsJhT",
        "outputId": "640fe9f7-dbe5-446c-ec3b-656c7289320e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Using data from Lecture because didn't have access to Kaggle data\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Dataset categories\n",
        "categories = ['sci.electronics',\n",
        "             'rec.sport.baseball',\n",
        "             'rec.sport.hockey']\n",
        "\n",
        "# Load training data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "# Load testing data\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                    remove=('headers', 'footers', 'quotes'),\n",
        "                                    categories=categories)\n",
        "\n",
        "print(f'Training Samples: {len(newsgroups_train.data)}')\n",
        "print(f'Testing Samples: {len(newsgroups_test.data)}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Samples: 1788\n",
            "Testing Samples: 1189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utbs3kgRmwjk"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# You may need to change the path\n",
        "train = fetch_20newsgroups(subset='train',\n",
        "                           remove=('headers', 'footers', 'quotes'),\n",
        "                           categories=categories)\n",
        "\n",
        "test = newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                            remove=('headers', 'footers', 'quotes'),\n",
        "                                            categories=categories)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ayBw5Gmwjk",
        "outputId": "3e3d23d7-758d-4c86-900c-8f1ffdb48814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train.data[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\nOh yeah, how come Dino could never take the Caps out of the Patrick\\nDivision?  He choked up 3 games to 1 last year and got swept away in\\nthe second round two years ago.  He rarely, if ever, makes it out of the\\ndivision.\\n\\n\\nSo are the Islanders, but they can still pull it out.  Vancouver has Winnipeg's\\n number, so it really doesn't matter.\\n\\n\\n\\n Kings always seem to go at least 6 or 7, they never play a four or five\\ngame serious.  There's a difference between battling it out and pulling it\\nout, as I take Calgary to pull it out in 7.\",\n",
              " 'Does anyone know where Billy Taylor is?  Richmond or Syracuse?  He was taken\\nby the Jays in the Rule V draft, but not kept on the roster.  Baseball Weekly\\nsaid that he was demoted to Syracuse, but a Toronto paper indicated that\\nthe Braves took him back.  Is there an Atlanta fan, or anyone reading this,\\nwho knows?   ',\n",
              " \"\\n\\n\\nWhy are you fooling around with analog for this job?  A single chip\\nmicro and a crystal will do the job reliably and easily.  An 8748 only\\ncosts about $5.  That and a $1 crystal and you're in business.  Embed\\nthe whole thing in a foam insulated blanket, power it from a solar cell,\\nuse the excess power to heat the assembly during the day and rely\\non the insulation to hold the heat during darkness.  If you don't want\\nto try thermal management, contact someone like ICL and have them cut\\nyou a special low temperature crystal.  It'll cost at most $20.\\n\\nIf you use a single chip micro, you're looking at a parts count of \\nmaybe 7.  A processor, a crystal, two caps on the crystal, a power FET\\nto fire the solenoid a flyback diode and a battery.  This is fewer parts than \\nyou can build an analog timer for and is infinitely more reliable.  Add\\na power zener diode (for heat) and a solar cell and the parts count\\nscreams up to 9.\\n\\nPD assemblers are available for all the common single chip micros.  This\\napplication is so trivial you could even look up the op codes in the \\nprogrammer's guide and create the binary with a hex editor.\\n\\nJohn\",\n",
              " '\\nCan anybody name a player who was \\'rushed\\' to the majors (let\\'s, for\\nargument\\'s sake, define \"rushed\" as brought up to the majors for more than\\na cup of coffee prior at age 22 or younger, and performing below\\nexpectations), whose career was damaged by this rushing?  I\\'m serious; I\\ntend to agree with David that bringing the player up sooner is better, but\\nI\\'d like to look at players for whom this theory didn\\'t work, if there are\\nany.  I\\'d prefer players within the last 10 years or so, because then I can\\nlook up their minor league stats.  (It\\'s important to distinguish between\\nplayers who legitimately had careers below what their minor league numbers\\nwould have projected, as opposed to players who were hyped and failed, but\\nactually had careers not out of line with their minor league numbers).  \\n\\nLet\\'s kick it off with an example of a player who was \"rushed\", although\\nthere doesn\\'t seem to have been any damage to his career.  Jay Bell was\\ngiven 135 PAs in the major leagues at age 21, and performed well below what\\nyou would expect from his AAA numbers the same season.  He got 236 PAs the\\nnext year at age 22, and still underperformed.  However, the next year, at\\nage 24, his performance improved, and he won the everyday shortstop job,\\nand has been there ever since.  It\\'s really hard for me to see where he\\nwould have been better off staying in the minor league (where he was\\nperformed quite well in AAA) during this time, rather than being \"rushed\";\\nCleveland might have been better off, I suppose, because they might have\\nbeen less likely to give up on him.\\n\\nYes, if you bring a player up early, he\\'s likely going to struggle.  But\\ndoes that delay the time at which he stops struggling, and starts\\nperforming up to expectations?',\n",
              " \"3. With Soderstrom and Roussel, why the hell would the Flyers want to\\n   pick up an older and slumping Roy?\\n\\n(BYW, I could come up with a group of players they'd trade for.... but\\nthey wouldn't be from the same team.)\\n\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jifvWV3Umwjm"
      },
      "source": [
        "### Define Pipeline Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAgSA-Ikmwjm",
        "outputId": "66b57170-8d36-49c3-c96d-74e11cc9451d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import the libraries\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english',\n",
        "                       ngram_range=(1,2),\n",
        "                       min_df=2,\n",
        "                       max_df=0.25)\n",
        "clf = LinearSVC()\n",
        "\n",
        "pipe = Pipeline([('vect', vect), ('clf', clf)])\n",
        "pipe"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.25, max_features=None,\n",
              "                                 min_df=2, ngram_range=(1, 2), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=None,\n",
              "                           tol=0.0001, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si128UoDmwjm"
      },
      "source": [
        "### Define Your Search Space\n",
        "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiJ2xadSmwjm",
        "outputId": "5b3778be-657d-4eab-e5f7-c0ae1870f0f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "parameters = {\n",
        "    'vect__max_df': (0.75, 1.0)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=4, verbose=1)\n",
        "grid_search.fit(train.data, train.target)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    6.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=0.25,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=2,\n",
              "                                                        ngram_range=(1, 2),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words='english',\n",
              "                                                        stri...\n",
              "                                        LinearSVC(C=1.0, class_weight=None,\n",
              "                                                  dual=True, fit_intercept=True,\n",
              "                                                  intercept_scaling=1,\n",
              "                                                  loss='squared_hinge',\n",
              "                                                  max_iter=1000,\n",
              "                                                  multi_class='ovr',\n",
              "                                                  penalty='l2',\n",
              "                                                  random_state=None, tol=0.0001,\n",
              "                                                  verbose=0))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=4,\n",
              "             param_grid={'vect__max_df': (0.75, 1.0)}, pre_dispatch='2*n_jobs',\n",
              "             refit=True, return_train_score=False, scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwE67swLmwjn"
      },
      "source": [
        "### Make a Submission File\n",
        "*Note:* In a typical Kaggle competition, you are only allowed two submissions a day, so you only submit if you feel you cannot achieve higher test accuracy. For this competition the max daily submissions are capped at **20**. Submit for each demo and for your assignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-70XZjwnpo",
        "outputId": "122f27c6-0398-4dca-d21c-32d8fa9f747c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9133217532823187"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx_bJUKcwp3H",
        "outputId": "e79e89eb-d30d-45e3-9044-e685172ad663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vect__max_df': 0.75}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrfk8Us0wqDI",
        "outputId": "25ea2ef0-69a0-454c-8b05-0e398feab950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate on test data\n",
        "y_test = grid_search.predict(test.data)\n",
        "accuracy_score(test.target, y_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test.target, y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88       397\n",
            "           1       0.91      0.89      0.90       399\n",
            "           2       0.94      0.92      0.93       393\n",
            "\n",
            "    accuracy                           0.90      1189\n",
            "   macro avg       0.91      0.90      0.90      1189\n",
            "weighted avg       0.91      0.90      0.90      1189\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg2y3uWKmwjn"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = grid_search.predict(test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4rh-58Jmwjn"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
        "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzCFqAIlmwjn"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTQp9imcmwjo"
      },
      "source": [
        "subNumber = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZuGhIzRmwjo"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "\n",
        "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
        "subNumber += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ygSGMGmwjo"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You're trying to achieve a minimum of 80% Accuracy on your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_hW3Rn6mwjo"
      },
      "source": [
        "## Latent Semantic Indexing (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "rOePO0Zsmwjo"
      },
      "source": [
        "## Follow Along\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model & try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
        "4. Make a submission to Kaggle \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1obSBvp7mwjp"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "lsi = Pipeline([('vect', vectorizer), ('svd', svd)])\n",
        "vect = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
        "clf = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "pipe = Pipeline([('lsi', lsi), ('clf', clf)])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eFZMlhmwjp"
      },
      "source": [
        "### Define Your Search Space\n",
        "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtjV6Wgymwjp",
        "outputId": "7c19a0d3-f678-4133-a740-5018b70a2a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "parameters = {\n",
        "    'lsi__svd__n_components': [10,100,250],\n",
        "    'lsi__vect__max_df': (0.75, 1.0),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=4, verbose=1)\n",
        "grid_search.fit(train.data, train.target)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('lsi',\n",
              "                                        Pipeline(memory=None,\n",
              "                                                 steps=[('vect',\n",
              "                                                         TfidfVectorizer(analyzer='word',\n",
              "                                                                         binary=False,\n",
              "                                                                         decode_error='strict',\n",
              "                                                                         dtype=<class 'numpy.float64'>,\n",
              "                                                                         encoding='utf-8',\n",
              "                                                                         input='content',\n",
              "                                                                         lowercase=True,\n",
              "                                                                         max_df=1.0,\n",
              "                                                                         max_features=None,\n",
              "                                                                         min_df=1,\n",
              "                                                                         ngram_range=(2,\n",
              "                                                                                      2),\n",
              "                                                                         norm='l2',\n",
              "                                                                         preprocessor=None,\n",
              "                                                                         smooth_...\n",
              "                                                           l1_ratio=None,\n",
              "                                                           max_iter=100,\n",
              "                                                           multi_class='auto',\n",
              "                                                           n_jobs=None,\n",
              "                                                           penalty='l2',\n",
              "                                                           random_state=None,\n",
              "                                                           solver='lbfgs',\n",
              "                                                           tol=0.0001,\n",
              "                                                           verbose=0,\n",
              "                                                           warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=4,\n",
              "             param_grid={'lsi__svd__n_components': [10, 100, 250],\n",
              "                         'lsi__vect__max_df': (0.75, 1.0)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpEgSiH22qh",
        "outputId": "a8840346-2ce8-41f0-b16b-0f6b87a48a2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7399402218988154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djfOqaVm23RM",
        "outputId": "08a8fea1-4498-4b49-a628-3f8c9b3d5499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lsi__svd__n_components': 250, 'lsi__vect__max_df': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdrcVrux23sQ",
        "outputId": "41d3e32c-03d6-485a-bd45-9bda2a8a24ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate on test data\n",
        "y_test = grid_search.predict(test.data)\n",
        "accuracy_score(test.target, y_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test.target, y_test))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.71      0.72       397\n",
            "           1       0.83      0.66      0.73       399\n",
            "           2       0.66      0.81      0.73       393\n",
            "\n",
            "    accuracy                           0.72      1189\n",
            "   macro avg       0.74      0.73      0.73      1189\n",
            "weighted avg       0.74      0.72      0.73      1189\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WwTPkarmwjq"
      },
      "source": [
        "### Make a Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sovw8fzSmwjq"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = grid_search.predict(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ofRQB7Ymwjq"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
        "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFklGe-bmwjq"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mLLYJf8mwjq"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "\n",
        "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
        "subNumber += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2ZL70vmwjr"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Continue to apply Latent Semantic Indexing (LSI) to various datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrFNuPu4mwjr"
      },
      "source": [
        "# Word Embeddings with Spacy (Learn)\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXnO_qJCmwjr"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InuPRFk4mwjr"
      },
      "source": [
        "# Apply to your Dataset\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_dist = {\n",
        "    \n",
        "    'max_depth' : randint(3,10),\n",
        "    'min_samples_leaf': randint(2,15)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUbiEwovmwjs"
      },
      "source": [
        "# Continue Word Embedding Work Here\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create the features and target\n",
        "sentences = test.data\n",
        "y = test.target\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "    sentences, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Function to return the vector for each sentence in a document\n",
        "def get_word_vectors(docs):\n",
        "    return [nlp(doc).vector for doc in docs]\n",
        "\n",
        "# Get the vectors for each sentence (mean of all the word vectors)\n",
        "X_train = get_word_vectors(sentences_train)\n",
        "X_test = get_word_vectors(sentences_test)\n",
        "\n",
        "# Instantiate the classifier (defaults)\n",
        "classifier = GradientBoostingClassifier()\n",
        "\n",
        "# Fit the model\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "# Print out the accuracy score\n",
        "print(\"Accuracy including word embeddings: \", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYaqXd0Fmwjs"
      },
      "source": [
        "### Make a Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgUY-_8zmwjs"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = ...predict(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srb2c3jFmwjs"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
        "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6m0l1mmmwjt"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emc0UMeLmwjt"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "\n",
        "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
        "subNumber += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fITOWS-xmwjt"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model & try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
        "    - Try to extract word embeddings with Spacy and use those embeddings as your features for a classification model.\n",
        "4. Make a submission to Kaggle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WNK5aeamwju"
      },
      "source": [
        "# Post Lecture Assignment\n",
        "<a id=\"p4\"></a>\n",
        "\n",
        "Your primary assignment this afternoon is to achieve a minimum of 80% accuracy on the Kaggle competition. Once you have achieved 70% accuracy, please work on the following: \n",
        "\n",
        "1. Research \"Sentiment Analysis\". Provide answers in markdown to the following questions: \n",
        "    - What is \"Sentiment Analysis\"? \n",
        "    - Is Document Classification different than \"Sentiment Analysis\"? Provide evidence for your response\n",
        "    - How do create labeled sentiment data? Are those labels really sentiment?\n",
        "    - What are common applications of sentiment analysis?\n",
        "2. Research our why word embeddings worked better for the lecture notebook than on the whiskey competition.\n",
        "    - This [text classification documentation](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) from Google might be of interest\n",
        "    - Neural Networks are becoming more popular for document classification. Why is that the case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oEBeZ-Omwju"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}